{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "* Geographical map of observations\n",
    "* Extract information from vectorizers (TF-IDF and Count)\n",
    "* group_by: keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first load in the saved objects, including both the vectorizers and the associated training datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in TF-IDF and Count Vectorizers and training datasets\n",
    "X_train_tfidf = load(\"X_train_tfidf.joblib\")\n",
    "X_train_count = load(\"X_train_count.joblib\")\n",
    "tfidf_vectorizer = load(\"tfidf_vectorizer.joblib\")\n",
    "count_vectorizer = load(\"count_vectorizer.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will extract the vocabulary/features from the vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample CountVectorizer features: ['00' '000' '0000' ... 'ûónegligence' 'ûótech' 'ûówe']\n",
      "Sample TF-IDF features: ['00' '000' '0000' ... 'ûónegligence' 'ûótech' 'ûówe']\n"
     ]
    }
   ],
   "source": [
    "# Get feature names (words)\n",
    "count_features = count_vectorizer.get_feature_names_out()\n",
    "tfidf_features = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"Sample CountVectorizer features:\", count_features)\n",
    "print(\"Sample TF-IDF features:\", tfidf_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, many of the features are noisy, such as IDs, hashes, and other random features. We will now extract the most frequent words and most relevant words, based on the CountVectorizer and TF-IDF Vectorizer approaches, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most frequent words:              word  count\n",
      "10005       http   4289\n",
      "13492    missing   2514\n",
      "14212        new    438\n",
      "10006      https    410\n",
      "12322       like    350\n",
      "23111         û_    346\n",
      "2138         amp    344\n",
      "11325       just    319\n",
      "21200        usa    281\n",
      "7134   emergency    262\n"
     ]
    }
   ],
   "source": [
    "# Sum word occurrences\n",
    "word_counts = np.asarray(X_train_count.sum(axis=0)).flatten()\n",
    "\n",
    "# Create df with words and their frequencies\n",
    "word_freq_df = pd.DataFrame({'word': count_features, 'count': word_counts})\n",
    "\n",
    "# Sort by frequency\n",
    "word_freq_df = word_freq_df.sort_values(by='count', ascending=False)\n",
    "\n",
    "print(\"Top 10 most frequent words: \", word_freq_df[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most important words:              word       tfidf\n",
      "10005       http  272.064231\n",
      "13492    missing  219.534227\n",
      "10006      https   61.679557\n",
      "14212        new   60.700329\n",
      "12322       like   56.467610\n",
      "11325       just   51.949189\n",
      "2138         amp   49.022791\n",
      "23111         û_   46.001873\n",
      "21200        usa   44.677352\n",
      "7134   emergency   44.382660\n"
     ]
    }
   ],
   "source": [
    "# Sum TF-IDF scores\n",
    "tfidf_weights = np.asarray(X_train_tfidf.sum(axis=0)).flatten()\n",
    "\n",
    "# Create df with words and their TF-IDF scores\n",
    "tfidf_df = pd.DataFrame({'word': tfidf_features, 'tfidf': tfidf_weights})\n",
    "\n",
    "# Sort by importance\n",
    "tfidf_df = tfidf_df.sort_values(by='tfidf', ascending=False)\n",
    "\n",
    "print(\"Top 10 most important words: \", tfidf_df[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will look at the importance of words/features, in relation to if they are a real vs. fake disaster tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             word  real_tfidf  fake_tfidf  difference\n",
      "10005        http  173.835672  139.441639   34.394033\n",
      "9760    hiroshima   22.452890    0.198542   22.254349\n",
      "4125   california   23.239443    0.994222   22.245221\n",
      "19452     suicide   23.459749    2.185977   21.273772\n",
      "13332       mh370   20.049027    0.000000   20.049027\n",
      "...           ...         ...         ...         ...\n",
      "6519          don   10.886780   31.540566  -20.653786\n",
      "3559         body    3.359401   25.313871  -21.954470\n",
      "10006       https   23.160087   51.086612  -27.926525\n",
      "12322        like   18.330455   49.323109  -30.992654\n",
      "11325        just   15.440651   46.908047  -31.467396\n",
      "\n",
      "[23168 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load in clean train and test sets\n",
    "clean_train = pd.read_pickle(\"clean_train.pkl\")\n",
    "clean_test = pd.read_pickle(\"clean_test.pkl\")\n",
    "\n",
    "# Separate real and fake tweets\n",
    "real_tweets = clean_train[clean_train['target'] == 1]['text']\n",
    "fake_tweets = clean_train[clean_train['target'] == 0]['text']\n",
    "\n",
    "# Vectorize separately\n",
    "X_real_tfidf = tfidf_vectorizer.transform(real_tweets)\n",
    "X_fake_tfidf = tfidf_vectorizer.transform(fake_tweets)\n",
    "\n",
    "# Compute sum of TF-IDF scores per word\n",
    "real_tfidf_scores = np.asarray(X_real_tfidf.sum(axis=0)).flatten()\n",
    "fake_tfidf_scores = np.asarray(X_fake_tfidf.sum(axis=0)).flatten()\n",
    "\n",
    "# Create DataFrame\n",
    "word_comparison_df = pd.DataFrame({\n",
    "    'word': tfidf_features,\n",
    "    'real_tfidf': real_tfidf_scores,\n",
    "    'fake_tfidf': fake_tfidf_scores\n",
    "})\n",
    "\n",
    "# Compute difference\n",
    "word_comparison_df['difference'] = word_comparison_df['real_tfidf'] - word_comparison_df['fake_tfidf']\n",
    "\n",
    "# Sort by words with greatest difference\n",
    "word_comparison_df = word_comparison_df.sort_values(by='difference', ascending=False)\n",
    "\n",
    "print(word_comparison_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how to interpret the above results:\n",
    "* High positive difference: Words are significantly more frequent in real disaster tweets\n",
    "* High negative difference: Words are significantly more freuqent in fake disaster tweets\n",
    "* Differnce close to 0: Words are equally present in both real and fake disaster tweets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
