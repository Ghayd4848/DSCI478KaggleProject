{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7069243156199678"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import load, dump\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load TFIDF Cleaned Data\n",
    "X_train_tfidf = load(\"model_train_tfidf.pkl\")\n",
    "y_train_tfidf = load(\"target.pkl\")\n",
    "\n",
    "X_test_tfidf = load(\"model_test_tfidf.pkl\")\n",
    "\n",
    "test_data = pd.read_csv(\"Data/test.csv\") # Load test dataset to get 'id' column\n",
    "\n",
    "# Split into training and validation sets (80% train, 20% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_tfidf, y_train_tfidf, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the XGBoost model with initial hyperparameters\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    scale_pos_weight=1.0,\n",
    "    eval_metric='logloss' \n",
    ")\n",
    "\n",
    "# Train model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# predictions on the validation set\n",
    "y_val_preds = xgb_model.predict(X_val)\n",
    "\n",
    "# F1 Score\n",
    "f1 = f1_score(y_val, y_val_preds)\n",
    "\n",
    "# Save the trained model\n",
    "model_path = \"xgb_model_tfidf.pkl\" \n",
    "dump(xgb_model, model_path)\n",
    "\n",
    "# Return F1 Score\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file preview:\n",
      "   id  target\n",
      "0   0       0\n",
      "1   2       1\n",
      "2   3       1\n",
      "3   9       0\n",
      "4  11       1\n"
     ]
    }
   ],
   "source": [
    "# Generate Predictions for Submission\n",
    "y_test_preds = xgb_model.predict(X_test_tfidf)\n",
    "\n",
    "# Match test_data to X_test_tfidf in size (remove extra rows if necessary)\n",
    "test_data_filtered = test_data.iloc[:len(y_test_preds)].copy()\n",
    "\n",
    "# Create Submission DataFrame\n",
    "submission = pd.DataFrame({'id': test_data_filtered['id'], 'target': y_test_preds})\n",
    "\n",
    "# Save Submission File\n",
    "submission.to_csv(\"xgboost_tfidf_submission.csv\", index=False)\n",
    "\n",
    "# Print first few rows for verification\n",
    "print(\"Submission file preview:\")\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF Model With Optuna Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-19 16:16:51,997] A new study created in memory with name: no-name-ac916f15-13c1-412d-8ff4-89568df89558\n",
      "[I 2025-02-19 16:16:56,398] Trial 0 finished with value: 0.677411206286573 and parameters: {'n_estimators': 218, 'learning_rate': 0.019632611406732395, 'max_depth': 5, 'subsample': 0.743051720575016, 'colsample_bytree': 0.6217964265129498, 'reg_alpha': 0.6892425127781228, 'reg_lambda': 4.551123878031604}. Best is trial 0 with value: 0.677411206286573.\n",
      "[I 2025-02-19 16:17:12,239] Trial 1 finished with value: 0.6906221608915578 and parameters: {'n_estimators': 400, 'learning_rate': 0.23754863954927946, 'max_depth': 9, 'subsample': 0.6194251776704998, 'colsample_bytree': 0.6369168862727841, 'reg_alpha': 0.44478075028015673, 'reg_lambda': 4.829031693908829}. Best is trial 1 with value: 0.6906221608915578.\n",
      "[I 2025-02-19 16:17:16,560] Trial 2 finished with value: 0.7111304805509343 and parameters: {'n_estimators': 149, 'learning_rate': 0.16545866320504052, 'max_depth': 6, 'subsample': 0.6241777673277757, 'colsample_bytree': 0.7545003073073416, 'reg_alpha': 0.0690586215490383, 'reg_lambda': 0.3573846620711213}. Best is trial 2 with value: 0.7111304805509343.\n",
      "[I 2025-02-19 16:17:20,232] Trial 3 finished with value: 0.6884865223049381 and parameters: {'n_estimators': 201, 'learning_rate': 0.06399948700293402, 'max_depth': 3, 'subsample': 0.6950203979724195, 'colsample_bytree': 0.8224470781995574, 'reg_alpha': 0.669180337544821, 'reg_lambda': 4.291519177413995}. Best is trial 2 with value: 0.7111304805509343.\n",
      "[I 2025-02-19 16:17:24,557] Trial 4 finished with value: 0.7104665955124574 and parameters: {'n_estimators': 149, 'learning_rate': 0.2191358891299336, 'max_depth': 7, 'subsample': 0.7661691167007838, 'colsample_bytree': 0.5412274236781491, 'reg_alpha': 0.5886350475301741, 'reg_lambda': 2.2913437777244168}. Best is trial 2 with value: 0.7111304805509343.\n",
      "[I 2025-02-19 16:17:29,484] Trial 5 finished with value: 0.7084429241461353 and parameters: {'n_estimators': 110, 'learning_rate': 0.09163440271639693, 'max_depth': 8, 'subsample': 0.5782053998464576, 'colsample_bytree': 0.9616655635274354, 'reg_alpha': 0.9455278337717226, 'reg_lambda': 0.5576168339418053}. Best is trial 2 with value: 0.7111304805509343.\n",
      "[I 2025-02-19 16:17:34,399] Trial 6 finished with value: 0.7051146888072669 and parameters: {'n_estimators': 319, 'learning_rate': 0.0745031855200612, 'max_depth': 4, 'subsample': 0.6053798175716131, 'colsample_bytree': 0.5996389945310437, 'reg_alpha': 0.9928003006059908, 'reg_lambda': 2.8081586581852447}. Best is trial 2 with value: 0.7111304805509343.\n",
      "[I 2025-02-19 16:17:37,039] Trial 7 finished with value: 0.7044812327349215 and parameters: {'n_estimators': 161, 'learning_rate': 0.20925158354222834, 'max_depth': 4, 'subsample': 0.654644687393023, 'colsample_bytree': 0.55251087537044, 'reg_alpha': 0.1429673838218889, 'reg_lambda': 4.580651668275078}. Best is trial 2 with value: 0.7111304805509343.\n",
      "[I 2025-02-19 16:17:47,799] Trial 8 finished with value: 0.6909904175854665 and parameters: {'n_estimators': 219, 'learning_rate': 0.020266239268709164, 'max_depth': 10, 'subsample': 0.5447407146188876, 'colsample_bytree': 0.6045864274057215, 'reg_alpha': 0.20035278687916092, 'reg_lambda': 4.57469380567354}. Best is trial 2 with value: 0.7111304805509343.\n",
      "[I 2025-02-19 16:17:56,775] Trial 9 finished with value: 0.713275223364891 and parameters: {'n_estimators': 242, 'learning_rate': 0.12125761259220297, 'max_depth': 10, 'subsample': 0.7265587193279748, 'colsample_bytree': 0.6364716671330038, 'reg_alpha': 0.47333959776999246, 'reg_lambda': 0.1734029862346964}. Best is trial 9 with value: 0.713275223364891.\n",
      "[I 2025-02-19 16:18:14,201] Trial 10 finished with value: 0.7048032471416835 and parameters: {'n_estimators': 458, 'learning_rate': 0.2884494343397569, 'max_depth': 10, 'subsample': 0.9244936023310774, 'colsample_bytree': 0.7230920660537948, 'reg_alpha': 0.38398217649686894, 'reg_lambda': 1.5287844640911443}. Best is trial 9 with value: 0.713275223364891.\n",
      "[I 2025-02-19 16:18:21,275] Trial 11 finished with value: 0.709233890556065 and parameters: {'n_estimators': 290, 'learning_rate': 0.14036253380946415, 'max_depth': 6, 'subsample': 0.8238052804621084, 'colsample_bytree': 0.7507212956455931, 'reg_alpha': 0.016140899638503126, 'reg_lambda': 0.007638704591962364}. Best is trial 9 with value: 0.713275223364891.\n",
      "[I 2025-02-19 16:18:29,523] Trial 12 finished with value: 0.7179327168397914 and parameters: {'n_estimators': 289, 'learning_rate': 0.15587134731818175, 'max_depth': 7, 'subsample': 0.862998612646322, 'colsample_bytree': 0.8259252447584499, 'reg_alpha': 0.2372918342531305, 'reg_lambda': 0.9282859574138145}. Best is trial 12 with value: 0.7179327168397914.\n",
      "[I 2025-02-19 16:18:40,547] Trial 13 finished with value: 0.7164178891840164 and parameters: {'n_estimators': 296, 'learning_rate': 0.1389175114221324, 'max_depth': 8, 'subsample': 0.8937324622800017, 'colsample_bytree': 0.8787909557110577, 'reg_alpha': 0.3339924616073545, 'reg_lambda': 1.1621625221869625}. Best is trial 12 with value: 0.7179327168397914.\n",
      "[I 2025-02-19 16:18:53,051] Trial 14 finished with value: 0.7164886308138599 and parameters: {'n_estimators': 353, 'learning_rate': 0.17901522051509938, 'max_depth': 8, 'subsample': 0.995884707827034, 'colsample_bytree': 0.922652821593801, 'reg_alpha': 0.2980083006392618, 'reg_lambda': 1.2108403011673226}. Best is trial 12 with value: 0.7179327168397914.\n",
      "[I 2025-02-19 16:19:05,101] Trial 15 finished with value: 0.7153353742769801 and parameters: {'n_estimators': 371, 'learning_rate': 0.1822857335915989, 'max_depth': 7, 'subsample': 0.9878275057997848, 'colsample_bytree': 0.9871571089995081, 'reg_alpha': 0.3114858208835778, 'reg_lambda': 1.8982174838693877}. Best is trial 12 with value: 0.7179327168397914.\n",
      "[I 2025-02-19 16:19:17,013] Trial 16 finished with value: 0.7139926598180283 and parameters: {'n_estimators': 360, 'learning_rate': 0.2555085590468903, 'max_depth': 8, 'subsample': 0.9942395975463061, 'colsample_bytree': 0.8987433459476853, 'reg_alpha': 0.25531558134747817, 'reg_lambda': 0.8525370585344261}. Best is trial 12 with value: 0.7179327168397914.\n",
      "[I 2025-02-19 16:19:35,845] Trial 17 finished with value: 0.7084959602958456 and parameters: {'n_estimators': 498, 'learning_rate': 0.19167112807840977, 'max_depth': 9, 'subsample': 0.8519767961327509, 'colsample_bytree': 0.8319376878538162, 'reg_alpha': 0.13776174268752603, 'reg_lambda': 3.1085365947718153}. Best is trial 12 with value: 0.7179327168397914.\n",
      "[I 2025-02-19 16:19:48,844] Trial 18 finished with value: 0.7145459980393413 and parameters: {'n_estimators': 404, 'learning_rate': 0.10238524980322389, 'max_depth': 7, 'subsample': 0.9332752804775247, 'colsample_bytree': 0.9232679676918043, 'reg_alpha': 0.5284513694211277, 'reg_lambda': 1.4369808800147426}. Best is trial 12 with value: 0.7179327168397914.\n",
      "[I 2025-02-19 16:19:55,387] Trial 19 finished with value: 0.7087687896726854 and parameters: {'n_estimators': 268, 'learning_rate': 0.1530055597503609, 'max_depth': 5, 'subsample': 0.8021356173568873, 'colsample_bytree': 0.8293629777537956, 'reg_alpha': 0.8359095217382025, 'reg_lambda': 1.979697446940179}. Best is trial 12 with value: 0.7179327168397914.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'n_estimators': 289, 'learning_rate': 0.15587134731818175, 'max_depth': 7, 'subsample': 0.862998612646322, 'colsample_bytree': 0.8259252447584499, 'reg_alpha': 0.2372918342531305, 'reg_lambda': 0.9282859574138145}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['xgb_model_optimized_tfidf.pkl']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score\n",
    "set.seed(478)\n",
    "def objective(trial):\n",
    "    # Define search space\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 1.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0, 5.0)\n",
    "    }\n",
    "\n",
    "    model = XGBClassifier(**params, eval_metric='logloss')\n",
    "    f1 = cross_val_score(model, X_train, y_train, cv=3, scoring='f1').mean()\n",
    "    \n",
    "    return f1\n",
    "\n",
    "# Optimize with Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best Hyperparameters:\", study.best_params)\n",
    "\n",
    "# Train final model using best parameters\n",
    "best_params = study.best_params\n",
    "xgb_model = XGBClassifier(**best_params, eval_metric='logloss')\n",
    "xgb_model.fit(X_train_tfidf, y_train_tfidf)\n",
    "\n",
    "# Save the optimized model\n",
    "dump(xgb_model, \"xgb_model_optimized_tfidf.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file preview:\n",
      "   id  target\n",
      "0   0       0\n",
      "1   2       1\n",
      "2   3       1\n",
      "3   9       0\n",
      "4  11       1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from joblib import load\n",
    "\n",
    "xgb_model = load(\"xgb_model_optimized_tfidf.pkl\")\n",
    "X_test_tfidf = load(\"model_test_tfidf.pkl\")\n",
    "\n",
    "y_test_preds = xgb_model.predict(X_test_tfidf)\n",
    "\n",
    "test_data = pd.read_csv(\"Data/test.csv\")\n",
    "test_data_filtered = test_data.iloc[:len(y_test_preds)].copy()\n",
    "\n",
    "submission = pd.DataFrame({'id': test_data_filtered['id'], 'target': y_test_preds})\n",
    "submission.to_csv(\"xgboost_submission_optimized_tfidf.csv\", index=False)\n",
    "\n",
    "print(\"Submission file preview:\")\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Vectorizer XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.719482619240097"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Count Vectorizer Cleaned Data\n",
    "X_train_count = load(\"model_train_count.pkl\")\n",
    "y_train_count = load(\"target.pkl\")\n",
    "\n",
    "X_test_count = load(\"model_test_count.pkl\")\n",
    "\n",
    "test_data = pd.read_csv(\"Data/test.csv\") # Load test dataset to get 'id' column \n",
    "\n",
    "# Split into training and validation sets (80% train, 20% validation)\n",
    "X_train_c, X_val_c, y_train_c, y_val_c = train_test_split(X_train_count, y_train_count, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the XGBoost model with initial hyperparameters\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    scale_pos_weight=1.0,\n",
    "    eval_metric='logloss'  \n",
    ")\n",
    "\n",
    "# Train model\n",
    "xgb_model.fit(X_train_c, y_train_c)\n",
    "\n",
    "# predictions on the validation set\n",
    "y_val_preds_c = xgb_model.predict(X_val_c)\n",
    "\n",
    "# F1 Score\n",
    "f1 = f1_score(y_val_c, y_val_preds_c)\n",
    "\n",
    "# Save the trained model\n",
    "model_path = \"xgb_model_count.pkl\"\n",
    "dump(xgb_model, model_path)\n",
    "\n",
    "# Return F1 Score\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file preview:\n",
      "   id  target\n",
      "0   0       1\n",
      "1   2       1\n",
      "2   3       1\n",
      "3   9       0\n",
      "4  11       1\n"
     ]
    }
   ],
   "source": [
    "# Generate Predictions for Submission\n",
    "y_test_preds_c = xgb_model.predict(X_test_count)\n",
    "\n",
    "# Match test_data to X_test_tfidf in size (remove extra rows if necessary)\n",
    "test_data_filtered = test_data.iloc[:len(y_test_preds_c)].copy()\n",
    "\n",
    "# Create Submission DataFrame\n",
    "submission = pd.DataFrame({'id': test_data_filtered['id'], 'target': y_test_preds_c})\n",
    "\n",
    "# Save Submission File\n",
    "submission.to_csv(\"xgboost_count_submission.csv\", index=False)\n",
    "\n",
    "# Print first few rows for verification\n",
    "print(\"Submission file preview:\")\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Vectorizer Model With Optuna Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-19 16:33:30,143] A new study created in memory with name: no-name-7c61a896-c655-4a35-a036-8a2a3bc6bf28\n",
      "[I 2025-02-19 16:33:35,294] Trial 0 finished with value: 0.7048207566362833 and parameters: {'n_estimators': 324, 'learning_rate': 0.03260404183588316, 'max_depth': 6, 'subsample': 0.8087635913896936, 'colsample_bytree': 0.8853897625559173, 'reg_alpha': 0.5597749354847169, 'reg_lambda': 2.1727844095531204}. Best is trial 0 with value: 0.7048207566362833.\n",
      "[I 2025-02-19 16:33:39,155] Trial 1 finished with value: 0.7053393624328775 and parameters: {'n_estimators': 386, 'learning_rate': 0.03506397512669123, 'max_depth': 4, 'subsample': 0.9323570087697401, 'colsample_bytree': 0.7297579072628659, 'reg_alpha': 0.8803378155994889, 'reg_lambda': 0.054430905773785176}. Best is trial 1 with value: 0.7053393624328775.\n",
      "[I 2025-02-19 16:33:44,198] Trial 2 finished with value: 0.7197301443932876 and parameters: {'n_estimators': 305, 'learning_rate': 0.10177518057726331, 'max_depth': 7, 'subsample': 0.5019847162520837, 'colsample_bytree': 0.861546545539879, 'reg_alpha': 0.45697060515308685, 'reg_lambda': 1.5567801471619769}. Best is trial 2 with value: 0.7197301443932876.\n",
      "[I 2025-02-19 16:33:49,079] Trial 3 finished with value: 0.7085467405820282 and parameters: {'n_estimators': 172, 'learning_rate': 0.19731298724593588, 'max_depth': 9, 'subsample': 0.514836472394615, 'colsample_bytree': 0.9965748440254245, 'reg_alpha': 0.43270379957054195, 'reg_lambda': 4.441508585017792}. Best is trial 2 with value: 0.7197301443932876.\n",
      "[I 2025-02-19 16:33:50,773] Trial 4 finished with value: 0.705538492263301 and parameters: {'n_estimators': 163, 'learning_rate': 0.06524103247437874, 'max_depth': 4, 'subsample': 0.6874023939831941, 'colsample_bytree': 0.5157177605624925, 'reg_alpha': 0.007188363962705169, 'reg_lambda': 0.40768986092173554}. Best is trial 2 with value: 0.7197301443932876.\n",
      "[I 2025-02-19 16:33:53,327] Trial 5 finished with value: 0.715185431099098 and parameters: {'n_estimators': 235, 'learning_rate': 0.27029063935449393, 'max_depth': 3, 'subsample': 0.7276911942941671, 'colsample_bytree': 0.8849548609756566, 'reg_alpha': 0.16412846598495912, 'reg_lambda': 4.648168410171078}. Best is trial 2 with value: 0.7197301443932876.\n",
      "[I 2025-02-19 16:33:57,624] Trial 6 finished with value: 0.7093722753733468 and parameters: {'n_estimators': 435, 'learning_rate': 0.07954265957585041, 'max_depth': 3, 'subsample': 0.6539505528724341, 'colsample_bytree': 0.8040439955904113, 'reg_alpha': 0.11323096931326038, 'reg_lambda': 4.260628243878843}. Best is trial 2 with value: 0.7197301443932876.\n",
      "[I 2025-02-19 16:34:01,943] Trial 7 finished with value: 0.7192007921550237 and parameters: {'n_estimators': 385, 'learning_rate': 0.17204179369436695, 'max_depth': 6, 'subsample': 0.9355101834173032, 'colsample_bytree': 0.527170740411016, 'reg_alpha': 0.1817525660891126, 'reg_lambda': 2.188401833690577}. Best is trial 2 with value: 0.7197301443932876.\n",
      "[I 2025-02-19 16:34:06,343] Trial 8 finished with value: 0.71483607031911 and parameters: {'n_estimators': 439, 'learning_rate': 0.09025890561492421, 'max_depth': 4, 'subsample': 0.6962862799558054, 'colsample_bytree': 0.6475820410611168, 'reg_alpha': 0.11639832010614881, 'reg_lambda': 3.6765404203589998}. Best is trial 2 with value: 0.7197301443932876.\n",
      "[I 2025-02-19 16:34:11,394] Trial 9 finished with value: 0.7164342289009603 and parameters: {'n_estimators': 215, 'learning_rate': 0.1268703126176522, 'max_depth': 8, 'subsample': 0.6807635715079936, 'colsample_bytree': 0.9714364931218485, 'reg_alpha': 0.5161551317647665, 'reg_lambda': 4.262994084941495}. Best is trial 2 with value: 0.7197301443932876.\n",
      "[I 2025-02-19 16:34:18,742] Trial 10 finished with value: 0.7084119367042421 and parameters: {'n_estimators': 288, 'learning_rate': 0.2089564044166643, 'max_depth': 10, 'subsample': 0.5031528543702175, 'colsample_bytree': 0.6891476678416601, 'reg_alpha': 0.8189242823701667, 'reg_lambda': 1.3456112150880082}. Best is trial 2 with value: 0.7197301443932876.\n",
      "[I 2025-02-19 16:34:23,489] Trial 11 finished with value: 0.7217283663174087 and parameters: {'n_estimators': 347, 'learning_rate': 0.15664721818819605, 'max_depth': 7, 'subsample': 0.9659296295141091, 'colsample_bytree': 0.5531855767589612, 'reg_alpha': 0.33494211101338334, 'reg_lambda': 2.339568184075054}. Best is trial 11 with value: 0.7217283663174087.\n",
      "[I 2025-02-19 16:34:28,145] Trial 12 finished with value: 0.717123788489765 and parameters: {'n_estimators': 309, 'learning_rate': 0.13057241239692938, 'max_depth': 7, 'subsample': 0.8411789706209372, 'colsample_bytree': 0.6182384688263585, 'reg_alpha': 0.351208557516447, 'reg_lambda': 2.862604764137939}. Best is trial 11 with value: 0.7217283663174087.\n",
      "[I 2025-02-19 16:34:30,347] Trial 13 finished with value: 0.717042284503155 and parameters: {'n_estimators': 102, 'learning_rate': 0.23439258734896806, 'max_depth': 7, 'subsample': 0.6005544887541552, 'colsample_bytree': 0.8276382347783016, 'reg_alpha': 0.683264482276926, 'reg_lambda': 0.968527665642507}. Best is trial 11 with value: 0.7217283663174087.\n",
      "[I 2025-02-19 16:34:36,199] Trial 14 finished with value: 0.7214944381205859 and parameters: {'n_estimators': 364, 'learning_rate': 0.13994538453208097, 'max_depth': 8, 'subsample': 0.9947367294318787, 'colsample_bytree': 0.6013221112592063, 'reg_alpha': 0.31860096535466004, 'reg_lambda': 2.8906365516645183}. Best is trial 11 with value: 0.7217283663174087.\n",
      "[I 2025-02-19 16:34:44,985] Trial 15 finished with value: 0.7217287250403374 and parameters: {'n_estimators': 490, 'learning_rate': 0.1650885294803864, 'max_depth': 9, 'subsample': 0.9999729126463341, 'colsample_bytree': 0.599342143572356, 'reg_alpha': 0.2946488601398795, 'reg_lambda': 3.1010803110061276}. Best is trial 15 with value: 0.7217287250403374.\n",
      "[I 2025-02-19 16:34:54,444] Trial 16 finished with value: 0.7191271539218596 and parameters: {'n_estimators': 498, 'learning_rate': 0.1725746657262478, 'max_depth': 10, 'subsample': 0.9917926387383048, 'colsample_bytree': 0.5762872634325195, 'reg_alpha': 0.2826710360701975, 'reg_lambda': 3.258652161169982}. Best is trial 15 with value: 0.7217287250403374.\n",
      "[I 2025-02-19 16:35:04,364] Trial 17 finished with value: 0.710723576034074 and parameters: {'n_estimators': 499, 'learning_rate': 0.2971333776794395, 'max_depth': 9, 'subsample': 0.8619094646324079, 'colsample_bytree': 0.5478480326285436, 'reg_alpha': 0.6571638855181721, 'reg_lambda': 3.6465694241423563}. Best is trial 15 with value: 0.7217287250403374.\n",
      "[I 2025-02-19 16:35:12,838] Trial 18 finished with value: 0.7177012903539798 and parameters: {'n_estimators': 437, 'learning_rate': 0.22724734897191773, 'max_depth': 8, 'subsample': 0.9338471358839276, 'colsample_bytree': 0.6705458991774105, 'reg_alpha': 0.2601161602944878, 'reg_lambda': 1.8386861166109587}. Best is trial 15 with value: 0.7217287250403374.\n",
      "[I 2025-02-19 16:35:16,699] Trial 19 finished with value: 0.7159317784212801 and parameters: {'n_estimators': 248, 'learning_rate': 0.1784392488429639, 'max_depth': 5, 'subsample': 0.7757017407113154, 'colsample_bytree': 0.7410912769454614, 'reg_alpha': 0.9913143063303054, 'reg_lambda': 2.797675148374325}. Best is trial 15 with value: 0.7217287250403374.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'n_estimators': 490, 'learning_rate': 0.1650885294803864, 'max_depth': 9, 'subsample': 0.9999729126463341, 'colsample_bytree': 0.599342143572356, 'reg_alpha': 0.2946488601398795, 'reg_lambda': 3.1010803110061276}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['xgb_model_optimized_count.pkl']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 1.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0, 5.0)\n",
    "    }\n",
    "\n",
    "    model = XGBClassifier(**params, eval_metric='logloss')\n",
    "    f1 = cross_val_score(model, X_train_c, y_train_c, cv=3, scoring='f1').mean()\n",
    "    \n",
    "    return f1\n",
    "\n",
    "# Optimize with Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best Hyperparameters:\", study.best_params)\n",
    "\n",
    "best_params = study.best_params\n",
    "xgb_model = XGBClassifier(**best_params, eval_metric='logloss')\n",
    "xgb_model.fit(X_train_c, y_train_c)\n",
    "\n",
    "dump(xgb_model, \"xgb_model_optimized_count.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file preview:\n",
      "   id  target\n",
      "0   0       0\n",
      "1   2       1\n",
      "2   3       1\n",
      "3   9       0\n",
      "4  11       1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from joblib import load\n",
    "\n",
    "xgb_model = load(\"xgb_model_optimized_count.pkl\")\n",
    "X_test_tfidf = load(\"model_test_count.pkl\")\n",
    "\n",
    "y_test_preds = xgb_model.predict(X_test_count)\n",
    "\n",
    "test_data = pd.read_csv(\"Data/test.csv\")\n",
    "test_data_filtered = test_data.iloc[:len(y_test_preds)].copy()\n",
    "\n",
    "submission = pd.DataFrame({'id': test_data_filtered['id'], 'target': y_test_preds})\n",
    "submission.to_csv(\"xgboost_submission_optimized_count.csv\", index=False)\n",
    "\n",
    "print(\"Submission file preview:\")\n",
    "print(submission.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
